{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QiaxukSMzf7w"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp  = spacy.blank('en')\n",
        "doc = nlp(\"Dr.Strange visited mumbai and he liked PavBhaji there. It cost only 2$ only\")\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcUYXRsT0VgI",
        "outputId": "8c7746df-8a01-4e37-ae97-450ed3400bda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "Strange\n",
            "visited\n",
            "mumbai\n",
            "and\n",
            "he\n",
            "liked\n",
            "PavBhaji\n",
            "there\n",
            ".\n",
            "It\n",
            "cost\n",
            "only\n",
            "2\n",
            "$\n",
            "only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp  = spacy.blank('en')\n",
        "doc = nlp('''\"Let's go to N.Y.!\"''')\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkCGg9ua0Vid",
        "outputId": "79d72ce2-d52c-4aa8-d61b-925e092a59bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "Let\n",
            "'s\n",
            "go\n",
            "to\n",
            "N.Y.\n",
            "!\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokens Attributes\n",
        "nlp  = spacy.blank('en')\n",
        "doc = nlp(\"Ram and shyam both having two $ !!!\")\n",
        "for token in doc:\n",
        "  print(f'Token = {token} | index = {token.i} | is_alpha = {token.is_alpha} | like_num = {token.like_num} | currency = {token.is_currency}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVPzb4sF0VlO",
        "outputId": "c2440dce-413a-463e-c7ec-12851805b3fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token = Ram | index = 0 | is_alpha = True | like_num = False | currency = False\n",
            "Token = and | index = 1 | is_alpha = True | like_num = False | currency = False\n",
            "Token = shyam | index = 2 | is_alpha = True | like_num = False | currency = False\n",
            "Token = both | index = 3 | is_alpha = True | like_num = False | currency = False\n",
            "Token = having | index = 4 | is_alpha = True | like_num = False | currency = False\n",
            "Token = two | index = 5 | is_alpha = True | like_num = True | currency = False\n",
            "Token = $ | index = 6 | is_alpha = False | like_num = False | currency = True\n",
            "Token = ! | index = 7 | is_alpha = False | like_num = False | currency = False\n",
            "Token = ! | index = 8 | is_alpha = False | like_num = False | currency = False\n",
            "Token = ! | index = 9 | is_alpha = False | like_num = False | currency = False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('students.txt') as f:\n",
        "  text = f.readlines()\n",
        "text = ' '.join(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gBr34hjy0VoC",
        "outputId": "e5c2c0a8-e0d2-4f89-a0c8-9ac4e510d556"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting email from text\n",
        "nlp = spacy.blank('en')\n",
        "doc = nlp(text)\n",
        "email = []\n",
        "for token in doc:\n",
        "  if token.like_email:\n",
        "    email.append(token.text)\n",
        "email"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEKOUyiG6VzH",
        "outputId": "ce3f4afa-a9d1-44a1-9f1d-9223b59a3533"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['virat@kohli.com',\n",
              " 'maria@sharapova.com',\n",
              " 'serena@williams.com',\n",
              " 'joe@root.com']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization in hindi\n",
        "nlp = spacy.blank('hi')\n",
        "doc = nlp(\"भैया जी! 5000 ₹ उधार थे वो वापस देदो\")\n",
        "for token in doc:\n",
        "  print(f'Token = {token} | currency = {token.is_currency} | num = {token.like_num}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdfCYBEi-p5e",
        "outputId": "64eb98ff-0714-4376-971e-2b26e4e2d6c4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token = भैया | currency = False | num = False\n",
            "Token = जी | currency = False | num = False\n",
            "Token = ! | currency = False | num = False\n",
            "Token = 5000 | currency = False | num = True\n",
            "Token = ₹ | currency = True | num = False\n",
            "Token = उधार | currency = False | num = False\n",
            "Token = थे | currency = False | num = False\n",
            "Token = वो | currency = False | num = False\n",
            "Token = वापस | currency = False | num = False\n",
            "Token = देदो | currency = False | num = False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.symbols import ORTH\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "tokens = [token.text for token in doc]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez9AN4ap_ks5",
        "outputId": "e64f04a4-48c9-4b44-ec9f-6278c3ae450d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.tokenizer.add_special_case(\"gimme\", [\n",
        "    {ORTH:'gim'},\n",
        "    {ORTH:'me'}\n",
        "])\n",
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "tokens = [token.text for token in doc]\n",
        "tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7kS-ymPA98k",
        "outputId": "17295e65-fdc6-44f4-fbb1-2f2274312bb0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank('en')\n",
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
        "for token in doc.sents:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "vnZeaiGqB71_",
        "outputId": "eb95b53d-2cd7-4e2c-c1aa-2f871c23728d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-b5e75fd25d03>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe('sentencizer')\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yTNHcquB74a",
        "outputId": "c8e0219f-41bf-4211-dd3f-c6b0682163ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sentencizer']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
        "for token in doc.sents:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QAVYVjVB766",
        "outputId": "3afd0860-a7e5-4528-fc36-46882f1a05f6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Strange loves pav bhaji of mumbai.\n",
            "Hulk loves chat of delhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**\n",
        "\n",
        "Task is to find url from text."
      ],
      "metadata": {
        "id": "bNA1DaXfCoKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "'''\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "tokens = [token for token in doc if token.like_url]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCWKduoBCry3",
        "outputId": "ee6bb777-14fa-4a62-c24a-cb9570b5b351"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[http://www.data.gov/,\n",
              " http://www.science,\n",
              " http://data.gov.uk/.,\n",
              " http://www3.norc.org/gss+website/,\n",
              " http://www.europeansocialsurvey.org/.]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#task 2 (Extract Money from text)\n",
        "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "doc = nlp(transactions)\n",
        "trans = []\n",
        "tokens= [token for token in doc]\n",
        "tokens\n",
        "for word in tokens:\n",
        "  if word.is_currency:\n",
        "    ans = str(tokens[(word.i)-1]) + word.text\n",
        "    trans.append(ans)\n",
        "trans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfQSCZrSC0rE",
        "outputId": "3ba480d1-ef63-4f09-a988-342afc519395"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['two$', '500€']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "om69E8jkFto2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}